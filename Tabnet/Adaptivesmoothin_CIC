import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, classification_report
import numpy as np
import sys
import os

RANDOM_SEED = 420

dataset = 'cic'

print(f'{dataset}')

# Load datasets
train = pd.read_csv(f'datasets\{dataset}_balanced_train_data.csv')
test = pd.read_csv(f'datasets\{dataset}_balanced_test_data.csv')

device = 'cpu'

# Separate features and target
if dataset == 'url':
    X_train = train.drop(columns=['label']).values
    y_train = train['label'].values
    X_test = test.drop(columns=['label']).values
    y_test = test['label'].values
else:
    X_train = train.drop(columns=['Label']).values
    y_train = train['Label'].values
    X_test = test.drop(columns=['Label']).values
    y_test = test['Label'].values

y_train_np = y_train.astype(np.int64)
y_test_np = y_test.astype(np.int64)


# # Convert the data to PyTorch tensors
X_train_tensor = torch.tensor(np.array(X_train), dtype=torch.float32).to(device) # Ensure X_train is converted to numpy
y_train_tensor = torch.tensor(np.array(y_train), dtype=torch.long).to(device)
X_test_tensor = torch.tensor(np.array(X_test), dtype=torch.float32).to(device)
y_test_tensor = torch.tensor(np.array(y_test), dtype=torch.long).to(device)

print(f"Train features tensor shape: {X_train_tensor.shape}")
print(f"Train labels tensor shape: {y_train_tensor.shape}")
print(f"Test features tensor shape: {X_test_tensor.shape}")

### Train a normal model to test it against the adversarail examples, to help us find the improvement in accuracy after the defense###
import time
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torch.utils.data import DataLoader
from scipy.stats import norm
from statsmodels.stats.proportion import proportion_confint
from math import ceil
from pytorch_tabnet.tab_model import TabNetClassifier

torch.manual_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)


if dataset == 'unsw':
    BATCH_SIZE=512
    NUM_EPOCHS=10
    param_grid = {
        'optimizer_fn': torch.optim.Adam,
        'optimizer_params': dict(lr=1e-3),
        'scheduler_params': {"step_size":10, "gamma":0.9},
        'scheduler_fn': torch.optim.lr_scheduler.StepLR,
        'input_dim': X_train.shape[1],
        'verbose': 1,
        'output_dim': 2,
        'device_name': device
        }
elif dataset == 'cic':
    #  CIC
    BATCH_SIZE=512
    NUM_EPOCHS = 10
    param_grid = {
        'optimizer_fn': torch.optim.Adam,
        'optimizer_params': dict(lr=1e-3),
        'scheduler_params': {"step_size":5, "gamma":0.5},
        'scheduler_fn': torch.optim.lr_scheduler.StepLR,
        'verbose': 1,
        'input_dim': X_train.shape[1],
        'output_dim': 2,
        'device_name': device
    }
elif dataset == 'url':
    ## URL 
    BATCH_SIZE=32
    NUM_EPOCHS = 50
    param_grid = {
        'optimizer_fn': torch.optim.Adam,
        'optimizer_params': dict(lr=1e-2),
        'scheduler_params': {"step_size":5, "gamma":0.1},
        'scheduler_fn': torch.optim.lr_scheduler.StepLR,
        'verbose': 1,
        'input_dim': X_train.shape[1],
        'output_dim': 2,
        'device_name': device
    }
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)


tabnet_model = TabNetClassifier(**param_grid)
tabnet_model._set_network()
tabnet_model._update_network_params()
tabnet_model._set_metrics(["accuracy"],["valid"])
tabnet_model.patience = 0
tabnet_model._set_optimizer()
tabnet_model._set_callbacks(None)
tabnet_model.update_fit_params(X_train_tensor, y_train_tensor,[(X_test_tensor,y_test_tensor)],1)


# Define loss function and optimizer
optimizer = tabnet_model._optimizer

criterion = tabnet_model._default_loss
scheduler = tabnet_model.scheduler_fn(optimizer, step_size=param_grid['scheduler_params']['step_size'], gamma=param_grid['scheduler_params']['gamma'])


# Training loop
for epoch in range(NUM_EPOCHS):
    tabnet_model.network.train()
    running_loss = 0.0
    running_sparsity_loss = 0.0  # Track M_loss
    
    for X_batch, y_batch in train_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)

        optimizer.zero_grad(set_to_none=True)  # Reset gradients

        # Forward pass
        outputs, M_loss = tabnet_model.network(X_batch)

        # Compute total loss
        loss = criterion(outputs, y_batch) - tabnet_model.lambda_sparse * M_loss
        
        # Backprop
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        running_sparsity_loss += abs(M_loss.item())  

    scheduler.step()
    
    avg_loss = running_loss / len(train_loader)
    avg_sparsity = running_sparsity_loss / len(train_loader)
    
    print(f"Epoch [{epoch+1}/{NUM_EPOCHS}] "
          f"Loss: {avg_loss:.4f}")

print("Training complete!")


results_file = f'tabnet_initial_performance_final_{dataset}.txt'
# Testing on clean test data
# with torch.no_grad():
#     X_test_tensor, y_test_tensor = X_test_tensor.to(device), y_test_tensor.to(device)
#     test_outputs, _ = tabnet_model.network(X_test_tensor)
#     test_predictions = torch.argmax(test_outputs, axis=1)
#     clean_accuracy = accuracy_score(y_test_tensor.cpu().numpy(), test_predictions.cpu().numpy())
#     print(f"Clean test accuracy: {clean_accuracy:.4f}")
#     with open(results_file, "a") as f:
#         f.write(f"\nClean test accuracy: {clean_accuracy:.4f}\n")




# Clipping function
def clip(current, low_bound, up_bound):
    # Convert bounds to tensors
    low_bound = torch.tensor(low_bound, dtype=torch.float32, device=current.device).unsqueeze(0)  # Add batch dimension
    up_bound = torch.tensor(up_bound, dtype=torch.float32, device=current.device).unsqueeze(0)    # Add batch dimension

    # Clip values
    clipped = torch.max(torch.min(current, up_bound), low_bound)
    return clipped



def lowProFool(x, model, weights, bounds, maxiters, alpha, lambda_, device):
    """
    LowProFool adversarial attack implementation with weighted loss.

    Args:
        x (torch.Tensor): Input sample.
        model (torch.nn.Module): Target model.
        weights (list or np.array): Feature importance weights.
        bounds (tuple): Feature bounds as (min, max).
        maxiters (int): Maximum number of iterations.
        alpha (float): Step size for gradient update.
        lambda_ (float): Regularization weight for L2 loss.
        device (torch.device): Device to run computations on.

    Returns:
        orig_pred (int): Original prediction class.
        output_pred (int): Adversarial prediction class.
        best_pert_x (np.array): Adversarial example.
    """
    # Initialize inputs and parameters
    x = x.unsqueeze(0).to(device).float()  # Ensure input is float32
    x.requires_grad = True
    r = torch.tensor(1e-4 * torch.ones_like(x), device=device, requires_grad=True)  # Perturbation
    v = torch.tensor(weights, device=device).float()  # Feature importance weights

    # Initial prediction
    logits, _ = model.network(x + r)
    orig_pred = logits.argmax(dim=1).item()

    # Define target class (opposite of original prediction)
    target_pred = 1 - orig_pred
    target = torch.tensor([target_pred], device=device).long()

    # Define loss functions
    criterion = torch.nn.CrossEntropyLoss()
    l2 = lambda v, r: torch.sqrt(torch.sum((v * r) ** 2))  # Weighted L2 norm

    best_norm_weighted = float('inf')
    best_pert_x = x.clone()

    # Iterative attack
    for loop_i in range(maxiters):
        if r.grad is not None:
            r.grad.zero_()

        # Compute logits and losses
        logits, _ = model.network(x + r)
        loss_1 = criterion(logits, target)  # Classification loss
        loss_2 = l2(v, r)  # Regularization loss
        loss = loss_1 + lambda_ * loss_2  # Total loss

        # Backpropagate the loss
        loss.backward()

        # Gradient update for perturbation
        with torch.no_grad():
            r -= alpha * r.grad  # Update perturbation

        # Clamp the perturbed sample within the bounds
        xprime = x + r
        bounds_min = torch.tensor(bounds[0], device=device).float()
        bounds_max = torch.tensor(bounds[1], device=device).float()
        xprime = torch.clamp(xprime, bounds_min, bounds_max)

        # Evaluate adversarial prediction
        logits, _ = model.network(xprime)
        output_pred = logits.argmax(dim=1).item()

        # Update best adversarial example based on weighted L2 norm
        weighted_norm = torch.sum(torch.abs(r * v)).item()
        if output_pred != orig_pred and weighted_norm < best_norm_weighted:
            best_norm_weighted = weighted_norm
            best_pert_x = xprime.clone()

    # Ensure the final adversarial example is clamped
    best_pert_x = torch.clamp(best_pert_x, bounds_min, bounds_max)
    logits, _ = model.network(best_pert_x)
    output_pred = logits.argmax(dim=1).item()

    return orig_pred, output_pred, best_pert_x.squeeze(0).detach().cpu().numpy()


### Train a normal model to test it against the adversarail examples, to help us find the improvement in accuracy after the defense###
import time
from art.attacks.evasion import ProjectedGradientDescent, CarliniL2Method, ZooAttack, HopSkipJump, FastGradientMethod, DeepFool, SaliencyMapMethod, LowProFool
from art.estimators.classification import PyTorchClassifier
from torch.utils.data import DataLoader, TensorDataset
import torch
import numpy as np
from torch.autograd import Variable
from scipy.stats import pearsonr
import torch.nn.functional as F
# ######## UNCOMMENT FOR ATTACKS ###################
class TabNetWrapper(nn.Module):
    def __init__(self, model):
        super(TabNetWrapper, self).__init__()
        self.model = model

    def forward(self, x, labels=None):
        # Ensure the model outputs raw logits (no softmax)
        outputs, _ = self.model.network(x)
        return outputs

    def parameters(self):
        return self.model.network.parameters()


# Model setup
input_size = X_train_tensor.shape[1]
model = TabNetWrapper(tabnet_model)

# Loss and optimizer
optimizer = tabnet_model._optimizer
criterion = nn.CrossEntropyLoss()


model = tabnet_model.network
# Ensure model and data are on the correct device
device = "cpu"


# Create a PyTorchClassifier for ART
classifier = PyTorchClassifier(
     model=model,
     clip_values=(0, 1),
     loss=criterion,
     optimizer=optimizer,
     input_shape=(X_test_tensor.shape[1],),
     nb_classes=2,
     device_type=device
 )

# Define attack parameters
zoo_attack = ZooAttack(
     classifier,
     confidence=0.8,  # Confidence of adversarial examples
     targeted=False,
     learning_rate=0.01,
     nb_parallel= 5,
     variable_h = 0.8,
     max_iter=40,  # Number of iterations
 )

cw_attack = CarliniL2Method(
     classifier=classifier,
     confidence=0.0,
     targeted=False,
     learning_rate=0.01,
     max_iter=10,  # Number of iterations
     binary_search_steps=10, # on cic it was 10
     initial_const=0.01,
 )

pgd = ProjectedGradientDescent(estimator=classifier, eps=0.15, eps_step=0.01, max_iter=40, norm=2)

deepfool = DeepFool(classifier, max_iter=10)

saliency = SaliencyMapMethod(classifier)

fgsm = FastGradientMethod(classifier, eps=0.15, norm=2)

hsj = HopSkipJump(classifier, max_iter=64, max_eval=10000, init_eval=100, init_size=100)

# Convert data to numpy
X_test_np = X_test_tensor.cpu().numpy()
y_test_np = y_test_tensor.cpu().numpy()

# Generate PGD adversarial examples
print("Generating PGD adversarial examples...")
X_adv_pgd = pgd.generate(X_test_np)
np.savetxt(f"X_adv_pgd_{dataset}.txt", X_adv_pgd, delimiter=",")
print(f"Saved PGD adversarial examples to 'X_adv_pgd_{dataset}.txt'.")

# # Generate FGSM adversarial examples
print("Generating FGSM adversarial examples...")
X_adv_fgsm = fgsm.generate(X_test_np)
np.savetxt(f"X_adv_fgsm_{dataset}.txt", X_adv_fgsm, delimiter=",")
print(f"Saved FGSM adversarial examples to 'X_adv_fgsm_{dataset}.txt'.")

# Generate DeepFool adversarial examples
print("Generating DeepFool adversarial examples...")
X_adv_df = deepfool.generate(X_test_np)
np.savetxt(f"X_adv_df_{dataset}.txt", X_adv_df, delimiter=",")
print(f"Saved DeepFool adversarial examples to 'X_adv_df_{dataset}.txt'.")

# Generate SaliencyMapMethod adversarial examples
print("Generating SaliencyMapMethod adversarial examples...")
X_adv_jsma = saliency.generate(X_test_np)
np.savetxt(f"X_adv_jsma_{dataset}.txt", X_adv_jsma, delimiter=",")
print(f"Saved JSMA adversarial examples to 'X_adv_jsma_{dataset}.txt'.")

print("Generating ZOO adversarial examples...")
X_adv_zoo = zoo_attack.generate(X_test_np)
np.savetxt(f"X_adv_zoo_agg_{dataset}.txt", X_adv_zoo, delimiter=",")
print(f"Saved ZOO adversarial examples to 'X_adv_zoo_{dataset}.txt'.")

feature_importance_weights = []
for i in range(X_train.shape[1]):
    if np.all(X_train[:, i] == X_train[0, i]):  # Check if the feature is constant
        feature_importance_weights.append(0.0)  # Assign zero weight to constant features
    else:
        correlation, _ = pearsonr(X_train[:, i], y_train)
        feature_importance_weights.append(abs(correlation))

feature_importance_weights = np.array(feature_importance_weights, dtype=np.float32)

# Define feature bounds based on the training data
feature_min = X_train.min(axis=0)
feature_max = X_train.max(axis=0)
feature_bounds = (feature_min, feature_max)

# Parameters for LowProFool attack
max_iters = 50
alpha = 0.005
lambda_ = 0.5

# Store adversarial examples
adversarial_examples = []
adversarial_accuracy = 0
num_samples = X_test.shape[0]  # Number of test samples

tabnet_model.network.eval()  # Set the model to evaluation mode
for i in range(num_samples):  # Iterate through each test sample
    print(f"Processing test sample {i + 1}/{num_samples}...")

    x_test_sample = X_test[i]
    x_test_sample_tensor = torch.tensor(x_test_sample, dtype=torch.float32).to(device)

    # Generate adversarial example using LowProFool attack
    orig_pred, adv_pred, adv_example = lowProFool(
        x_test_sample_tensor, tabnet_model, feature_importance_weights, feature_bounds, max_iters, alpha, lambda_, device
    )

    adversarial_examples.append(adv_example)

    if orig_pred == adv_pred:
        adversarial_accuracy += 1


# Convert adversarial examples to a numpy array
adversarial_examples = np.array(adversarial_examples)

# Save all adversarial examples to a single file
np.savetxt(f"X_adv_lpf_{dataset}.csv", adversarial_examples, delimiter=",")

# Calculate adversarial accuracy
adversarial_accuracy /= num_samples
print(f'Adversarial Accuracy: {adversarial_accuracy * 100:.2f}%')

# Generate HSJ adversarial examples
print("Generating HSJ adversarial examples...")
X_adv_hsj = hsj.generate(X_test_np)
np.savetxt(f"X_adv_hsj_{dataset}.txt", X_adv_hsj, delimiter=",")
print(f"Saved HSJ adversarial examples to 'X_adv_hsj_{dataset}.txt'.")

# Generate CW adversarial examples
print("Generating CW adversarial examples...")
X_adv_cw = cw_attack.generate(X_test_np)
np.savetxt(f"X_adv_cw_agg_{dataset}.txt", X_adv_cw, delimiter=",")
print(f"Saved CW adversarial examples to 'X_adv_cw_{dataset}.txt'.")


adversarial_loaders = {
    "JSMA": f"X_adv_jsma_{dataset}.txt",
    "DF": f"X_adv_df_{dataset}.txt",
    "ZOO": f"X_adv_zoo_agg_{dataset}.txt",
    "CW": f"X_adv_cw_agg_{dataset}.txt",
    "LPF": f"X_adv_lpf_{dataset}.csv",
    "HSJ": f"X_adv_hsj_{dataset}.txt",
    "PGD2": f"X_adv_pgd_{dataset}_norm2.txt",
    "FGSM2": f"X_adv_fgsm_{dataset}_norm2.txt"
}

# Evaluate on adversarial examples
for attack_name, file_path in adversarial_loaders.items():
    # Load adversarial data
    X_adv = np.loadtxt(file_path, delimiter=",")  
    X_adv_tensor = torch.tensor(X_adv, dtype=torch.float32).to(device)


    # Record evaluation time
    start_time = time.time()
    with torch.no_grad():
        adv_outputs, _ = tabnet_model.network(X_adv_tensor)
        adv_predictions = torch.argmax(adv_outputs, axis=1)
    end_time = time.time()

    # Compute adversarial accuracy
    adv_accuracy = accuracy_score(y_test_tensor.cpu().numpy(), adv_predictions.cpu().numpy())
    eval_time = end_time - start_time

    # Print and save results
    print(f"NC -- {attack_name} adversarial accuracy: {adv_accuracy:.4f}")
    # print(f"Time taken for {attack_name} evaluation: {eval_time:.4f} seconds")
    with open(results_file, "a") as f:
        f.write(f"NC -- {attack_name} adversarial accuracy: {adv_accuracy:.4f}\n")
        # f.write(f"Time taken for {attack_name} evaluation: {eval_time:.4f} seconds\n")

adversarial_loaders = {
    "JSMA": f"X_adv_jsma_{dataset}_restored_M.csv",
    "DF": f"X_adv_df_{dataset}_restored_M.csv",
    "ZOO": f"X_adv_zoo_{dataset}_restored_M.csv",
    "CW": f"X_adv_cw_{dataset}_restored_M.csv",
    "LPF": f"X_adv_lpf_{dataset}_restored_M.csv",
    "HSJ": f"X_adv_hsj_{dataset}_restored_M.csv",
    "PGD2": f"X_adv_pgd_{dataset}_restored_M_norm2.csv",
    "FGSM2": f"X_adv_fgsm_{dataset}_restored_M_norm2.csv"
}

# # Evaluate on adversarial examples
for attack_name, file_path in adversarial_loaders.items():
    # Load adversarial data
    X_adv = np.loadtxt(file_path, delimiter=",")  # Load the CSV as a NumPy array
    X_adv_tensor = torch.tensor(X_adv, dtype=torch.float32).to(device)

    # Record evaluation time
    start_time = time.time()
    with torch.no_grad():
        adv_outputs, _ = tabnet_model.network(X_adv_tensor)
        adv_predictions = torch.argmax(adv_outputs, axis=1)
    end_time = time.time()

    # Compute adversarial accuracy
    adv_accuracy = accuracy_score(y_test_tensor.cpu().numpy(), adv_predictions.cpu().numpy())
    eval_time = end_time - start_time

    # Print and save results
    print(f"C -- {attack_name} adversarial accuracy: {adv_accuracy:.4f}")
    # print(f"Time taken for {attack_name} evaluation: {eval_time:.4f} seconds")
    with open(results_file, "a") as f:
        f.write(f"C -- {attack_name} adversarial accuracy: {adv_accuracy:.4f}\n")
        # f.write(f"Time taken for {attack_name} evaluation: {eval_time:.4f} seconds\n")


unique, counts = np.unique(y_train, return_counts=True)

# Print label distribution
print("Class Distribution:", dict(zip(unique, counts)))

sigma = 0.01

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torch.utils.data import DataLoader
from scipy.stats import norm
from statsmodels.stats.proportion import proportion_confint
from math import ceil
import time

class AdaptiveSmoothEntropy:
    ABSTAIN = -1

    def __init__(self, base_classifier, num_classes, initial_sigma=1):
        self.base_classifier = base_classifier
        self.num_classes = num_classes
        self.sigma = initial_sigma
        self.margin_logs = []

    def train_model(self, train_loader, test_loader, num_epochs=30):
        optimizer = self.base_classifier._optimizer
        criterion = self.base_classifier._default_loss
        scheduler = self.base_classifier.scheduler_fn(optimizer, step_size=param_grid['scheduler_params']['step_size'], gamma=param_grid['scheduler_params']['gamma'])

        for epoch in range(num_epochs):
            running_loss = 0.0
            self.base_classifier.network.train()
            for inputs, labels in train_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                for param in self.base_classifier.network.parameters():
                    param.grad = None

                if epoch < num_epochs // 2:
                    outputs_clean, M_loss = self.base_classifier.network(inputs)  # Unpack TabNet output

                    loss = criterion(outputs_clean, labels)
                    loss = loss - self.base_classifier.lambda_sparse * M_loss
                else:
                    clean_inputs, noisy_inputs = self.mix_clean_and_noisy(inputs)
                    outputs_clean, M_loss_clean = self.base_classifier.network(clean_inputs)
                    outputs_noisy, M_loss_noisy = self.base_classifier.network(noisy_inputs)
                    loss_clean = criterion(outputs_clean, labels)
                    loss_clean = loss_clean - self.base_classifier.lambda_sparse * M_loss_clean
                    loss_noisy = criterion(outputs_noisy, labels)
                    loss_noisy = loss_noisy - self.base_classifier.lambda_sparse * M_loss_noisy
                    loss = loss_clean + loss_noisy

                loss.backward()
                optimizer.step()
                running_loss += loss.item()

            scheduler.step()
            print(f"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}")

        self.evaluate(test_loader)

    def mix_clean_and_noisy(self, inputs):
        clean_inputs = inputs.clone()
        with torch.no_grad():
            # probs = torch.softmax(self.base_classifier(clean_inputs), dim=1)
            # entropies = -torch.sum(probs * torch.log(probs + 1e-8), dim=1)
            probs = torch.tensor(self.base_classifier.predict_proba(clean_inputs.cpu())).to(clean_inputs.device)
            entropies = -torch.sum(probs * torch.log(probs + 1e-8), dim=1)
        adaptive_noise_scale = 1 - (entropies / np.log(self.num_classes))
        noise = torch.randn_like(inputs) * self.sigma * adaptive_noise_scale.unsqueeze(1)
        noisy_inputs = inputs + noise
        return clean_inputs, noisy_inputs

    def evaluate(self, loader):
        self.base_classifier.network.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for inputs, labels in loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs, _ = self.base_classifier.network(inputs)
                predicted = torch.argmax(outputs, dim=1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        accuracy = 100 * correct / total
        print(f"Accuracy on clean data: {accuracy:.2f}%")
        return accuracy

    def _proxy_margin(self, x):
        with torch.no_grad():
            
            # probs = torch.tensor(self.base_classifier.predict_proba(x.cpu()), dim=1).squeeze()
            probs = torch.tensor(self.base_classifier.predict_proba(x.cpu())).squeeze()
        top2 = torch.topk(probs, 2).values
        return (top2[0] - top2[1]).item(), top2[0].item()

    def _adaptive_sample_size(self, margin, confidence):
        scale = max(1.0, (1.0 - margin) * (1.0 - confidence)) * 2
        return int(ceil(1000 * scale))

    def certify(self, x, n0, n_base, alpha, batch_size, sample_id=None):
        self.base_classifier.network.eval()
        
        try:
            counts_selection = self._sample_noise(x, n0, batch_size)
            cAHat = counts_selection.argmax().item()

            margin, confidence = self._proxy_margin(x)
            n = self._adaptive_sample_size(margin, confidence)

            counts_estimation = self._sample_noise(x, n, batch_size)
            nA = counts_estimation[cAHat].item()
            pABar = self._lower_confidence_bound(nA, n, alpha)

            if pABar < 0.5:
                predicted_class = AdaptiveSmoothEntropy.ABSTAIN
                radius = 0.0
            else:
                predicted_class = cAHat
                radius = self.sigma * norm.ppf(pABar)

            return predicted_class, radius
        except Exception as e:
            print(f"Error: {e}")
            return AdaptiveSmoothEntropy.ABSTAIN, 0.0

    def _sample_noise(self, x, num, batch_size):
        counts = np.zeros(self.num_classes, dtype=int)
        with torch.no_grad():
            
            for _ in range(ceil(num / batch_size)):
                this_batch_size = min(batch_size, num)
                num -= this_batch_size
                batch = x.repeat((this_batch_size, 1))
                noise = torch.randn_like(batch) * self.sigma
                predictions = self.base_classifier.predict(batch + noise)
                counts += np.bincount(predictions.cpu().numpy(), minlength=self.num_classes)
        return counts

    def _lower_confidence_bound(self, NA, N, alpha):
        return proportion_confint(NA, N, alpha=2 * alpha, method="beta")[0]

    def test_on_adversarial(self, X_adv_test, y_test, method="margin", n0=100, n=1000, alpha=0.001, batch_size=64):
        total = len(X_adv_test)
        correct = 0
        certified_correct = 0

        with torch.no_grad():
            for i in range(total):
                x_adv_sample = X_adv_test[i].clone().detach().unsqueeze(0).to(device)
                y_true = y_test[i].item()

                certified_class, certified_radius = self.certify(
                    x_adv_sample, n0=n0, n_base=n, alpha=alpha, batch_size=batch_size, sample_id=i
                )
                predicted = torch.tensor(self.base_classifier.predict(x_adv_sample))

                if predicted == y_true:
                    correct += 1
                if certified_class == y_true:
                    certified_correct += 1

        accuracy = correct / total * 100
        certified_accuracy = certified_correct / total * 100
        print(f"{method.capitalize()} Accuracy on adversarial examples: {accuracy:.2f}%")
        print(f"{method.capitalize()} Certified accuracy: {certified_accuracy:.2f}%")
        return accuracy, certified_accuracy


###########ADAPTIVE MODEL##############
def train_and_evaluate_on_adversarial_folds(
    base_model_class,
    adaptive_model_class,
    X_train_tensor,
    y_train_tensor,
    X_test_tensor,
    y_test_tensor,
    adversarial_loaders,
    num_classes,
    initial_sigma,
    batch_size=64,
    num_epochs=30,
    folds=5,
    output_file="upd_adapted_adversarial_evaluation_results.txt",
   ):
    import torch
    from torch.utils.data import DataLoader, TensorDataset, Subset
    from sklearn.model_selection import KFold
    import numpy as np
    from collections import defaultdict
    import os
    import time

    device = "cpu"

    # Convert testing dataset to PyTorch DataLoader
    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
    kf = KFold(n_splits=folds, shuffle=True, random_state=42)

    # Prepare output file
    with open(output_file, "w") as f:
        f.write("===== Adversarial Evaluation Results =====\n")
        f.write(f"===== initial_sigma = {initial_sigma}")

    # Initialize and train the base model
    print("Initializing models...")
    # base_model = base_model_class(input_size=X_train_tensor.shape[1], dropout_rate=0.5).to(device)
    base_model = base_model_class
    adaptive_model = adaptive_model_class(
        base_classifier=base_model,
        num_classes=num_classes,
        initial_sigma=initial_sigma,
    )

    # Training on full training dataset
    print("Training AdaptiveSmoothEntropy model...")
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    adaptive_model.train_model(train_loader, test_loader, num_epochs=num_epochs)

    # Evaluate on clean testing data
    print("Evaluating AdaptiveSmoothEntropy model on clean testing data...")
    clean_accuracy = adaptive_model.evaluate(test_loader)
    print(f"Clean Test Accuracy: {clean_accuracy:.2f}%")
    with open(output_file, "a") as f:
        f.write(f"\nClean Test Accuracy: {clean_accuracy:.2f}%\n")

    # Evaluate adversarial examples using folds
    fold_results = []
    timing_results = []
    adversarial_accuracies_summary = defaultdict(list)

    for fold, (_, test_idx) in enumerate(kf.split(test_dataset)):
        print(f"\n===== Fold {fold + 1}/{folds} =====")

        val_subset = Subset(test_dataset, test_idx)
        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)

        # y_val = torch.stack([val_subset[i][1] for i in range(len(val_subset))]).cpu().numpy()
        y_val = torch.stack([val_subset[i][1] for i in range(len(val_subset))]).cpu().numpy()

        fold_adversarial_accuracies = {}
        fold_timing = {}

        for attack_name, loader in adversarial_loaders.items():
            print(f"\nEvaluating {attack_name} adversarial examples on Fold {fold + 1}...")

            # Load adversarial examples corresponding to the fold
            X_adv_loaded = np.loadtxt(loader, delimiter=",")
            X_adv_tensor = torch.tensor(X_adv_loaded[test_idx], dtype=torch.float32).to(device)

            # Time evaluation
            start_time = time.time()
            # Test on adversarial examples
            accuracy, certified_accuracy = adaptive_model.test_on_adversarial(
                X_adv_tensor,
                y_val,
                method="margin",
                n0=100,
                n=1000,
                alpha=0.001,
                batch_size=batch_size,
            )
            elapsed_time = time.time() - start_time

            fold_adversarial_accuracies[attack_name] = {
                "accuracy": accuracy,
                "certified_accuracy": certified_accuracy,
            }
            fold_timing[attack_name] = elapsed_time
            adversarial_accuracies_summary[attack_name].append((accuracy, certified_accuracy))

        # Log fold results
        with open(output_file, "a") as f:
            f.write(f"\nFold {fold + 1}:\n")
            f.write(f"  Clean Accuracy: {clean_accuracy:.2f}%\n")
            for attack_name, metrics in fold_adversarial_accuracies.items():
                f.write(f"  {attack_name}: Accuracy = {metrics['accuracy']:.2f}%, Certified Accuracy = {metrics['certified_accuracy']:.2f}%\n")
                f.write(f"  {attack_name}: Time = {fold_timing[attack_name]:.2f} seconds\n")

        fold_results.append({
            "fold": fold + 1,
            "adversarial_accuracies": fold_adversarial_accuracies,
        })
        timing_results.append(fold_timing)

    avg_timing = {
        attack_name: np.mean([timing[attack_name] for timing in timing_results])
        for attack_name in adversarial_loaders.keys()
    }

    # Final Summary
    print("\n===== Adversarial Evaluation Summary =====")
    with open(output_file, "a") as f:
        f.write("\n===== Adversarial Evaluation Summary =====\n")
        for attack_name, metrics in adversarial_accuracies_summary.items():
            avg_accuracy = np.mean([m[0] for m in metrics])
            avg_certified_accuracy = np.mean([m[1] for m in metrics])
            print(f"{attack_name}: Average Accuracy = {avg_accuracy:.2f}%, Average Certified Accuracy = {avg_certified_accuracy:.2f}%")
            f.write(f"{attack_name}: Average Accuracy = {avg_accuracy:.2f}%, Certified Accuracy = {avg_certified_accuracy:.2f}%\n")

        # Log average timing
        print("\n===== Average Timing Summary =====")
        f.write("\n===== Average Timing Summary =====\n")
        for attack_name, avg_time in avg_timing.items():
            print(f"  {attack_name}: Average Time = {avg_time:.2f} seconds")
            f.write(f"  {attack_name}: Average Time = {avg_time:.2f} seconds\n")

  
    return fold_results


tabnet_model = TabNetClassifier(**param_grid)
tabnet_model._set_network()
tabnet_model._update_network_params()
tabnet_model._set_metrics(["accuracy"],["valid"])
tabnet_model.patience = 0
tabnet_model._set_optimizer()
tabnet_model._set_callbacks(None)
tabnet_model.update_fit_params(X_train_tensor, y_train_tensor,[(X_test_tensor,y_test_tensor)],1)


results = train_and_evaluate_on_adversarial_folds(
    base_model_class=tabnet_model,
    adaptive_model_class=AdaptiveSmoothEntropy,
    X_train_tensor=X_train_tensor,
    y_train_tensor=y_train_tensor,
    X_test_tensor=X_test_tensor,
    y_test_tensor=y_test_tensor,
    adversarial_loaders=adversarial_loaders,
    num_classes=2,
    initial_sigma=sigma,
    batch_size=BATCH_SIZE,
    num_epochs=NUM_EPOCHS,
    folds=2,
    output_file=f"results/M_updated_{dataset}_adaptive_adversarial_evaluation_results_norm2.txt",
)


import torch
import os
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from scipy.stats import norm
from statsmodels.stats.proportion import proportion_confint
import numpy as np
from math import ceil
from pytorch_tabnet.tab_model import TabNetClassifier



# Original Randomized Smoothing (certification method)
class Smooth:
    ABSTAIN = -1

    def __init__(self, base_classifier, num_classes: int, sigma: float):
        self.base_classifier = base_classifier
        self.num_classes = num_classes
        self.sigma = sigma

    def certify(self, x: torch.tensor, n0: int, n: int, alpha: float, batch_size: int):
        self.base_classifier.network.eval()
        try:
            counts_selection = self._sample_noise(x, n0, batch_size)
            cAHat = counts_selection.argmax().item()
            counts_estimation = self._sample_noise(x, n, batch_size)
            nA = counts_estimation[cAHat].item()
            pABar = self._lower_confidence_bound(nA, n, alpha)
            if pABar < 0.5:
                predicted_class = Smooth.ABSTAIN
                radius = 0.0
            else:
                predicted_class = cAHat
                radius = self.sigma * norm.ppf(pABar)

            # Save results to a text file
            file_path = f"results/{dataset}_{sigma}_original_smoothing_results.txt"
            os.makedirs(os.path.dirname(file_path), exist_ok=True)  
            with open(file_path, "a") as file:
                file.write(f"Predicted Class: {predicted_class}, Radius: {radius}, Abstain: {predicted_class == Smooth.ABSTAIN}\n")

            return predicted_class, radius
        except Exception as e:
                print(f"Error during certification: {e}")
                return Smooth.ABSTAIN, 0.0

    def _sample_noise(self, x: torch.tensor, num: int, batch_size: int):
        with torch.no_grad():
            counts = np.zeros(self.num_classes, dtype=int)
            for _ in range(ceil(num / batch_size)):
                this_batch_size = min(batch_size, num)
                num -= this_batch_size
                batch = x.repeat((this_batch_size, 1))
                noise = torch.randn_like(batch) * self.sigma
                logits, _ = self.base_classifier.network(batch + noise)  
                predictions = logits.argmax(1)  
                counts += np.bincount(predictions.cpu().numpy(), minlength=self.num_classes)
            return counts

    def _lower_confidence_bound(self, NA: int, N: int, alpha: float) -> float:
        return proportion_confint(NA, N, alpha=2 * alpha, method="beta")[0]




  # def train_model(self, model, train_loader, criterion, noise_sd, num_epochs=10):
    def train_model(self, train_loader, test_loader, noise_sd, num_epochs=10):
        optimizer = self.base_classifier._optimizer
        criterion = self.base_classifier._default_loss
        
        for epoch in range(num_epochs):
            self.base_classifier.network.train()
            running_loss = 0.0
            for inputs, labels in train_loader:
                inputs, labels = inputs.to(device), labels.to(device)  # Move data to the correct device
                inputs.requires_grad = True  # Enable gradients for inputs

                for param in self.base_classifier.network.parameters():
                    param.grad = None

                optimizer.zero_grad()

                # Add noise to the inputs
                noisy_inputs = inputs + torch.randn_like(inputs) * noise_sd

                # Forward pass through the TabNet network
                logits, M_loss = self.base_classifier.network(noisy_inputs)  # Extract logits from the TabNet network

                # Compute loss
                loss = criterion(logits, labels)
                loss = loss - self.base_classifier.lambda_sparse * M_loss

                # Backward pass and optimization
                loss.backward()
                optimizer.step()

                running_loss += loss.item()


            # Print epoch loss
            print(f"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(train_loader):.4f}")

        self.base_classifier.network.eval()
        # Evaluate model performance on test data
        self.evaluate_model(test_loader, noise_sd)


    # Evaluation function
    def evaluate_model(self, test_loader, noise_sd):
        self.base_classifier.network.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                noisy_inputs = inputs + torch.randn_like(inputs) * noise_sd
                outputs, _ = self.base_classifier.network(noisy_inputs)

                predictions = outputs.argmax(1)
                correct += (predictions == labels).sum().item()
                total += labels.size(0)
            accuracy = 100 * correct / total # Calculate accuracy
            print(f"Accuracy on clean data: {accuracy:.2f}%")
            return accuracy # Return the calculated accuracy


    def test_on_adversarial(self, X_adv_test, y_test, n0=100, n=1000, alpha=0.001, batch_size=64):
        """Test model on adversarial examples with certification."""
        total = len(X_adv_test)
        correct = 0
        certified_correct = 0

        with torch.no_grad():
            for i in range(total):
                x_adv_sample = X_adv_test[i].clone().detach().unsqueeze(0).to(device)
                y_true = y_test[i].item()

                certified_class, certified_radius = self.certify(
                    x_adv_sample, n0=n0, n=n, alpha=alpha, batch_size=batch_size
                )

                outputs, _ = self.base_classifier.network(x_adv_sample)
                predicted = outputs.argmax(1)

                if predicted.item() == y_true:
                    correct += 1
                if certified_class == y_true:
                    certified_correct += 1

        accuracy = correct / total * 100
        certified_accuracy = certified_correct / total * 100
        print(f" Accuracy on adversarial examples: {accuracy:.2f}%")
        print(f" Certified accuracy: {certified_accuracy:.2f}%")
        return accuracy, certified_accuracy

def train_and_evaluate_on_adversarial_folds(
    base_model_class,
    adaptive_model_class,
    X_train_tensor,
    y_train_tensor,
    X_test_tensor,
    y_test_tensor,
    adversarial_loaders,
    num_classes,
    initial_sigma,
    batch_size=64,
    num_epochs=10,
    folds=5,
    output_file=f"unsw_adversarial_evaluation_results_originalSmoothing.txt",
):
    import torch
    from torch.utils.data import DataLoader, TensorDataset, Subset
    from sklearn.model_selection import KFold
    import numpy as np
    from collections import defaultdict
    import time

    # device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Convert testing dataset to PyTorch DataLoader
    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
    kf = KFold(n_splits=folds, shuffle=True, random_state=42)

    # Prepare output file
    with open(output_file, "w") as f:
        f.write("===== Adversarial Evaluation Results =====\n")
        f.write(f"===== initial_sigma = {initial_sigma}")

    # Initialize and train the base model
    print("Initializing models...")
    # base_model = base_model_class(input_size=X_train_tensor.shape[1], dropout_rate=0.5).to(device)
    base_model = base_model_class

    adaptive_model = adaptive_model_class(
        base_classifier=base_model,
        num_classes=num_classes,
        sigma=initial_sigma
    )

    # Training on full training dataset
    print("Training OriginalSmooth model...")
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)


    noise_sd = 0.5

    adaptive_model.train_model(train_loader, test_loader, noise_sd, num_epochs)

    # Evaluate on clean testing data
    print("Evaluating OriginalSmooth model on clean testing data...")
    clean_accuracy = adaptive_model.evaluate_model(test_loader, noise_sd=noise_sd)
    print(f"Clean Test Accuracy: {clean_accuracy:.2f}%")
    with open(output_file, "a") as f:
        f.write(f"\nClean Test Accuracy: {clean_accuracy:.2f}%\n")

    # Evaluate adversarial examples using folds
    fold_results = []
    timing_results = []
    adversarial_accuracies_summary = defaultdict(list)

    for fold, (_, test_idx) in enumerate(kf.split(test_dataset)):
        print(f"\n===== Fold {fold + 1}/{folds} =====")

        val_subset = Subset(test_dataset, test_idx)
        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)

        y_val = torch.stack([val_subset[i][1] for i in range(len(val_subset))]).to(device)

        fold_adversarial_accuracies = {}
        fold_timing = {}
        for attack_name, loader in adversarial_loaders.items():
            print(f"\nEvaluating {attack_name} adversarial examples on Fold {fold + 1}...")

            # Load adversarial examples corresponding to the fold
            X_adv_loaded = np.loadtxt(loader, delimiter=",")
            X_adv_tensor = torch.tensor(X_adv_loaded[test_idx], dtype=torch.float32, device=device)

             # Time evaluation
            start_time = time.time()
            # Test on adversarial examples
            accuracy, certified_accuracy = adaptive_model.test_on_adversarial(
                X_adv_tensor,
                y_val,
                n0=100,
                n=1000,
                alpha=0.001,
                batch_size=batch_size,
            )
            elapsed_time = time.time() - start_time

            fold_adversarial_accuracies[attack_name] = {
                "accuracy": accuracy,
                "certified_accuracy": certified_accuracy,
            }
            fold_timing[attack_name] = elapsed_time
            adversarial_accuracies_summary[attack_name].append((accuracy, certified_accuracy))

        # Log fold results
        with open(output_file, "a") as f:
            f.write(f"\nFold {fold + 1}:\n")
            f.write(f"  Clean Accuracy: {clean_accuracy:.2f}%\n")
            for attack_name, metrics in fold_adversarial_accuracies.items():
                f.write(f"  {attack_name}: Accuracy = {metrics['accuracy']:.2f}%, Certified Accuracy = {metrics['certified_accuracy']:.2f}%\n")
                f.write(f"  {attack_name}: Time = {fold_timing[attack_name]:.2f} seconds\n")


        fold_results.append({
            "fold": fold + 1,
            "adversarial_accuracies": fold_adversarial_accuracies,
        })
        timing_results.append(fold_timing)

        avg_timing = {attack_name: np.mean([timing[attack_name] for timing in timing_results]) for attack_name in adversarial_loaders.keys()}

    # Calculate and print averages
    print("\n===== Adversarial Evaluation Summary =====")
    with open(output_file, "a") as f:
        f.write("\n===== Adversarial Evaluation Summary =====\n")

        for attack_name, metrics in adversarial_accuracies_summary.items():
            avg_accuracy = np.mean([m[0] for m in metrics])
            avg_certified_accuracy = np.mean([m[1] for m in metrics])

            print(f"{attack_name}: Average Accuracy = {avg_accuracy:.2f}%, Average Certified Accuracy = {avg_certified_accuracy:.2f}%")
            f.write(f"{attack_name}: Average Accuracy = {avg_accuracy:.2f}%, Average Certified Accuracy = {avg_certified_accuracy:.2f}%\n")

                # Log average timing
        print("\n===== Average Timing Summary =====")
        f.write("\n===== Average Timing Summary =====\n")
        for attack_name, avg_time in avg_timing.items():
            print(f"  {attack_name}: Average Time = {avg_time:.2f} seconds")
            f.write(f"  {attack_name}: Average Time = {avg_time:.2f} seconds\n")

    return fold_results




tabnet_model = TabNetClassifier(**param_grid)
tabnet_model._set_network()
tabnet_model._update_network_params()
tabnet_model._set_metrics(["accuracy"],["valid"])
tabnet_model.patience = 0
tabnet_model._set_optimizer()
tabnet_model._set_callbacks(None)
tabnet_model.update_fit_params(X_train_tensor, y_train_tensor,[(X_test_tensor,y_test_tensor)],1)


results = train_and_evaluate_on_adversarial_folds(
    base_model_class=tabnet_model,
    adaptive_model_class=Smooth, #original
    X_train_tensor=X_train_tensor,  # Training data
    y_train_tensor=y_train_tensor,  # Training labels
    X_test_tensor=X_test_tensor,    # Testing data
    y_test_tensor=y_test_tensor,    # Testing labels
    adversarial_loaders=adversarial_loaders,
    num_classes=2,
    initial_sigma=sigma,
    batch_size=BATCH_SIZE,
    num_epochs=NUM_EPOCHS,
    folds=2,
    output_file=f"{dataset}_adversarial_evaluation_results_originalSmoothing.txt",
)

X_test_np = X_test_tensor.cpu().numpy()
np.savetxt("X_test_clean.txt", X_test_np, delimiter=",")
